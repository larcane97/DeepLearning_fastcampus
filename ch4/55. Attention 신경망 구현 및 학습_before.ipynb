{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention 신경망 구현 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "NUM_WORDS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
    "        self.lstm = tf.keras.layers.LSTM(512, return_state=True)\n",
    "\n",
    "    def call(self, x, training=False, mask=None):\n",
    "        x = self.emb(x)\n",
    "        _, h, c = self.lstm(x)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
    "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        x, h, c = inputs\n",
    "        x = self.emb(x)\n",
    "        x, h, c = self.lstm(x, initial_state=[h, c])\n",
    "        return self.dense(x), h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seq(tf.keras.Model):\n",
    "    def __init__(self, sos, eos):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decoder()\n",
    "        self.sos = sos\n",
    "        self.eos = eos\n",
    "\n",
    "    def call(self, inputs, training=False, mask=None):\n",
    "        if training is True:\n",
    "            x, y = inputs\n",
    "            h, c = self.enc(x)\n",
    "            y, _, _ = self.dec((y, h, c))\n",
    "            return y\n",
    "        else:\n",
    "            x = inputs\n",
    "            h, c = self.enc(x)\n",
    "            y = tf.convert_to_tensor(self.sos)\n",
    "            y = tf.reshape(y, (1, 1))\n",
    "\n",
    "            seq = tf.TensorArray(tf.int32, 64)\n",
    "\n",
    "            for idx in tf.range(64):\n",
    "                y, h, c = self.dec([y, h, c])\n",
    "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
    "                y = tf.reshape(y, (1, 1))\n",
    "                seq = seq.write(idx, y)\n",
    "\n",
    "                if y == self.eos:\n",
    "                    break\n",
    "\n",
    "            return tf.reshape(seq.stack(), (1, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습, 테스트 루프 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement training loop\n",
    "@tf.function\n",
    "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    output_labels = labels[:, 1:]\n",
    "    shifted_labels = labels[:, :-1]\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model([inputs, shifted_labels], training=True)\n",
    "        loss = loss_object(output_labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(output_labels, predictions)\n",
    "\n",
    "# Implement algorithm test\n",
    "@tf.function\n",
    "def test_step(model, inputs):\n",
    "    return model(inputs, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeyong/.conda/envs/tf2/lib/python3.7/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "dataset_file = 'chatbot_data.csv'  # acquired from 'http://www.aihub.or.kr' and modified\n",
    "okt = Okt()\n",
    "\n",
    "with open(dataset_file, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    seq = [' '.join(okt.morphs(line)) for line in lines]\n",
    "\n",
    "questions = seq[::2]\n",
    "answers = ['\\t ' + lines for lines in seq[1::2]]\n",
    "\n",
    "num_sample = len(questions)\n",
    "\n",
    "perm = list(range(num_sample))\n",
    "random.seed(0)\n",
    "random.shuffle(perm)\n",
    "\n",
    "train_q = list()\n",
    "train_a = list()\n",
    "test_q = list()\n",
    "test_a = list()\n",
    "\n",
    "for idx, qna in enumerate(zip(questions, answers)):\n",
    "    q, a = qna\n",
    "    if perm[idx] > num_sample//5:\n",
    "        train_q.append(q)\n",
    "        train_a.append(a)\n",
    "    else:\n",
    "        test_q.append(q)\n",
    "        test_a.append(a)\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,\n",
    "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
    "\n",
    "tokenizer.fit_on_texts(train_q + train_a)\n",
    "\n",
    "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
    "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
    "\n",
    "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
    "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='pre',\n",
    "                                                        maxlen=64)\n",
    "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=65)\n",
    "\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='pre',\n",
    "                                                       maxlen=64)\n",
    "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=65)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 환경 정의\n",
    "### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Seq2seq(sos=tokenizer.word_index['\\t'],\n",
    "                eos=tokenizer.word_index['\\n'])\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define performance metrics\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0922 09:42:08.512909 140618365970240 deprecation.py:323] From /home/jeyong/.conda/envs/tf2/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.0273499488830566, Accuracy: 89.98668670654297\n",
      "Epoch 2, Loss: 0.6014062166213989, Accuracy: 90.96961212158203\n",
      "Epoch 3, Loss: 0.5653008222579956, Accuracy: 91.04401397705078\n",
      "Epoch 4, Loss: 0.5494084358215332, Accuracy: 91.11841583251953\n",
      "Epoch 5, Loss: 0.5412740111351013, Accuracy: 91.10667419433594\n",
      "Epoch 6, Loss: 0.5360599160194397, Accuracy: 91.08708953857422\n",
      "Epoch 7, Loss: 0.5315815210342407, Accuracy: 91.12625122070312\n",
      "Epoch 8, Loss: 0.5269421339035034, Accuracy: 91.1614990234375\n",
      "Epoch 9, Loss: 0.5218955278396606, Accuracy: 91.20065307617188\n",
      "Epoch 10, Loss: 0.5152804851531982, Accuracy: 91.2202377319336\n",
      "Epoch 11, Loss: 0.5063177347183228, Accuracy: 91.23981475830078\n",
      "Epoch 12, Loss: 0.49390506744384766, Accuracy: 91.28681182861328\n",
      "Epoch 13, Loss: 0.47931432723999023, Accuracy: 91.44737243652344\n",
      "Epoch 14, Loss: 0.46515291929244995, Accuracy: 91.72541046142578\n",
      "Epoch 15, Loss: 0.4509647786617279, Accuracy: 92.089599609375\n",
      "Epoch 16, Loss: 0.44248345494270325, Accuracy: 92.2775650024414\n",
      "Epoch 17, Loss: 0.43532487750053406, Accuracy: 92.35588836669922\n",
      "Epoch 18, Loss: 0.4286859929561615, Accuracy: 92.42637634277344\n",
      "Epoch 19, Loss: 0.42304182052612305, Accuracy: 92.481201171875\n",
      "Epoch 20, Loss: 0.41838014125823975, Accuracy: 92.52427673339844\n",
      "Epoch 21, Loss: 0.4144025146961212, Accuracy: 92.59868621826172\n",
      "Epoch 22, Loss: 0.41353288292884827, Accuracy: 92.61434936523438\n",
      "Epoch 23, Loss: 0.4128120243549347, Accuracy: 92.61434936523438\n",
      "Epoch 24, Loss: 0.40164658427238464, Accuracy: 92.64958953857422\n",
      "Epoch 25, Loss: 0.3955984115600586, Accuracy: 92.71224975585938\n",
      "Epoch 26, Loss: 0.39222875237464905, Accuracy: 92.75140380859375\n",
      "Epoch 27, Loss: 0.3887656033039093, Accuracy: 92.78665161132812\n",
      "Epoch 28, Loss: 0.38538700342178345, Accuracy: 92.82972717285156\n",
      "Epoch 29, Loss: 0.382290244102478, Accuracy: 92.86497497558594\n",
      "Epoch 30, Loss: 0.37923479080200195, Accuracy: 92.90021514892578\n",
      "Epoch 31, Loss: 0.3761677145957947, Accuracy: 92.93546295166016\n",
      "Epoch 32, Loss: 0.37309524416923523, Accuracy: 92.99812316894531\n",
      "Epoch 33, Loss: 0.3700093924999237, Accuracy: 93.04511260986328\n",
      "Epoch 34, Loss: 0.36699458956718445, Accuracy: 93.08818817138672\n",
      "Epoch 35, Loss: 0.36388275027275085, Accuracy: 93.11952209472656\n",
      "Epoch 36, Loss: 0.3608263432979584, Accuracy: 93.16259765625\n",
      "Epoch 37, Loss: 0.3576243221759796, Accuracy: 93.2213363647461\n",
      "Epoch 38, Loss: 0.35444119572639465, Accuracy: 93.23699951171875\n",
      "Epoch 39, Loss: 0.3511541187763214, Accuracy: 93.27616119384766\n",
      "Epoch 40, Loss: 0.34794875979423523, Accuracy: 93.30357360839844\n",
      "Epoch 41, Loss: 0.34467771649360657, Accuracy: 93.33881378173828\n",
      "Epoch 42, Loss: 0.3414715528488159, Accuracy: 93.37406158447266\n",
      "Epoch 43, Loss: 0.33838656544685364, Accuracy: 93.42497253417969\n",
      "Epoch 44, Loss: 0.3354671597480774, Accuracy: 93.44454956054688\n",
      "Epoch 45, Loss: 0.33304256200790405, Accuracy: 93.43672180175781\n",
      "Epoch 46, Loss: 0.33161047101020813, Accuracy: 93.47196197509766\n",
      "Epoch 47, Loss: 0.331102579832077, Accuracy: 93.46804809570312\n",
      "Epoch 48, Loss: 0.3285684883594513, Accuracy: 93.53853607177734\n",
      "Epoch 49, Loss: 0.33080625534057617, Accuracy: 93.48371124267578\n",
      "Epoch 50, Loss: 0.325813353061676, Accuracy: 93.55419921875\n",
      "Epoch 51, Loss: 0.31740114092826843, Accuracy: 93.66776275634766\n",
      "Epoch 52, Loss: 0.31070229411125183, Accuracy: 93.73042297363281\n",
      "Epoch 53, Loss: 0.3066093623638153, Accuracy: 93.80482482910156\n",
      "Epoch 54, Loss: 0.30262038111686707, Accuracy: 93.84398651123047\n",
      "Epoch 55, Loss: 0.2989037334918976, Accuracy: 93.89881134033203\n",
      "Epoch 56, Loss: 0.2954263985157013, Accuracy: 93.94580078125\n",
      "Epoch 57, Loss: 0.2921273410320282, Accuracy: 93.98104858398438\n",
      "Epoch 58, Loss: 0.2890574336051941, Accuracy: 94.06719970703125\n",
      "Epoch 59, Loss: 0.2866368889808655, Accuracy: 94.10636138916016\n",
      "Epoch 60, Loss: 0.2829507291316986, Accuracy: 94.13768768310547\n",
      "Epoch 61, Loss: 0.2778420150279999, Accuracy: 94.20426177978516\n",
      "Epoch 62, Loss: 0.2739325761795044, Accuracy: 94.25908660888672\n",
      "Epoch 63, Loss: 0.270027220249176, Accuracy: 94.33348846435547\n",
      "Epoch 64, Loss: 0.2680874168872833, Accuracy: 94.35698699951172\n",
      "Epoch 65, Loss: 0.26744920015335083, Accuracy: 94.3452377319336\n",
      "Epoch 66, Loss: 0.26717206835746765, Accuracy: 94.36090087890625\n",
      "Epoch 67, Loss: 0.26527631282806396, Accuracy: 94.38831329345703\n",
      "Epoch 68, Loss: 0.25655514001846313, Accuracy: 94.52537536621094\n",
      "Epoch 69, Loss: 0.24805647134780884, Accuracy: 94.76033782958984\n",
      "Epoch 70, Loss: 0.24289362132549286, Accuracy: 94.88957214355469\n",
      "Epoch 71, Loss: 0.2385665327310562, Accuracy: 94.92872619628906\n",
      "Epoch 72, Loss: 0.23458531498908997, Accuracy: 95.05012512207031\n",
      "Epoch 73, Loss: 0.23093603551387787, Accuracy: 95.14019775390625\n",
      "Epoch 74, Loss: 0.22725853323936462, Accuracy: 95.2067642211914\n",
      "Epoch 75, Loss: 0.223844975233078, Accuracy: 95.26942443847656\n",
      "Epoch 76, Loss: 0.22107337415218353, Accuracy: 95.33599853515625\n",
      "Epoch 77, Loss: 0.2188618779182434, Accuracy: 95.3712387084961\n",
      "Epoch 78, Loss: 0.21658018231391907, Accuracy: 95.39082336425781\n",
      "Epoch 79, Loss: 0.21336929500102997, Accuracy: 95.47697448730469\n",
      "Epoch 80, Loss: 0.2082311064004898, Accuracy: 95.63752746582031\n",
      "Epoch 81, Loss: 0.20328126847743988, Accuracy: 95.73151397705078\n",
      "Epoch 82, Loss: 0.19927968084812164, Accuracy: 95.85682678222656\n",
      "Epoch 83, Loss: 0.19605612754821777, Accuracy: 95.9586410522461\n",
      "Epoch 84, Loss: 0.19293776154518127, Accuracy: 96.01738739013672\n",
      "Epoch 85, Loss: 0.18976256251335144, Accuracy: 96.11920928955078\n",
      "Epoch 86, Loss: 0.18645912408828735, Accuracy: 96.19361114501953\n",
      "Epoch 87, Loss: 0.18273036181926727, Accuracy: 96.26409912109375\n",
      "Epoch 88, Loss: 0.1797756850719452, Accuracy: 96.29151153564453\n",
      "Epoch 89, Loss: 0.1769147664308548, Accuracy: 96.41682434082031\n",
      "Epoch 90, Loss: 0.17318251729011536, Accuracy: 96.5421371459961\n",
      "Epoch 91, Loss: 0.17039163410663605, Accuracy: 96.61653900146484\n",
      "Epoch 92, Loss: 0.1667349636554718, Accuracy: 96.67919921875\n",
      "Epoch 93, Loss: 0.16338878870010376, Accuracy: 96.68702697753906\n",
      "Epoch 94, Loss: 0.160878524184227, Accuracy: 96.79276275634766\n",
      "Epoch 95, Loss: 0.1574014574289322, Accuracy: 96.90632629394531\n",
      "Epoch 96, Loss: 0.15481247007846832, Accuracy: 96.90632629394531\n",
      "Epoch 97, Loss: 0.15195105969905853, Accuracy: 96.95332336425781\n",
      "Epoch 98, Loss: 0.14879712462425232, Accuracy: 97.04338836669922\n",
      "Epoch 99, Loss: 0.1464938223361969, Accuracy: 97.12171173095703\n",
      "Epoch 100, Loss: 0.14388233423233032, Accuracy: 97.21177673339844\n",
      "Epoch 101, Loss: 0.14099380373954773, Accuracy: 97.23136138916016\n",
      "Epoch 102, Loss: 0.1385703682899475, Accuracy: 97.2666015625\n",
      "Epoch 103, Loss: 0.13601668179035187, Accuracy: 97.3253402709961\n",
      "Epoch 104, Loss: 0.13343004882335663, Accuracy: 97.38016510009766\n",
      "Epoch 105, Loss: 0.13122917711734772, Accuracy: 97.46631622314453\n",
      "Epoch 106, Loss: 0.1291629672050476, Accuracy: 97.5015640258789\n",
      "Epoch 107, Loss: 0.12653158605098724, Accuracy: 97.57596588134766\n",
      "Epoch 108, Loss: 0.12427954375743866, Accuracy: 97.59555053710938\n",
      "Epoch 109, Loss: 0.12214326858520508, Accuracy: 97.63079071044922\n",
      "Epoch 110, Loss: 0.1199159100651741, Accuracy: 97.66604614257812\n",
      "Epoch 111, Loss: 0.11779633909463882, Accuracy: 97.71694946289062\n",
      "Epoch 112, Loss: 0.11589674651622772, Accuracy: 97.76786041259766\n",
      "Epoch 113, Loss: 0.11342453956604004, Accuracy: 97.8109359741211\n",
      "Epoch 114, Loss: 0.11131927371025085, Accuracy: 97.85009765625\n",
      "Epoch 115, Loss: 0.10941135883331299, Accuracy: 97.87359619140625\n",
      "Epoch 116, Loss: 0.10743541270494461, Accuracy: 97.95191192626953\n",
      "Epoch 117, Loss: 0.10548175871372223, Accuracy: 97.95191192626953\n",
      "Epoch 118, Loss: 0.10360220074653625, Accuracy: 98.00282287597656\n",
      "Epoch 119, Loss: 0.1016625314950943, Accuracy: 98.02239990234375\n",
      "Epoch 120, Loss: 0.09992340207099915, Accuracy: 98.08113861083984\n",
      "Epoch 121, Loss: 0.09801050275564194, Accuracy: 98.08897399902344\n",
      "Epoch 122, Loss: 0.0963253602385521, Accuracy: 98.11247253417969\n",
      "Epoch 123, Loss: 0.09468787163496017, Accuracy: 98.143798828125\n",
      "Epoch 124, Loss: 0.09341104328632355, Accuracy: 98.14771270751953\n",
      "Epoch 125, Loss: 0.0913390964269638, Accuracy: 98.18687438964844\n",
      "Epoch 126, Loss: 0.089755579829216, Accuracy: 98.24952697753906\n",
      "Epoch 127, Loss: 0.0881836861371994, Accuracy: 98.26911163330078\n",
      "Epoch 128, Loss: 0.08640006929636002, Accuracy: 98.29261016845703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129, Loss: 0.08461929112672806, Accuracy: 98.34351348876953\n",
      "Epoch 130, Loss: 0.0824970081448555, Accuracy: 98.36701202392578\n",
      "Epoch 131, Loss: 0.0811024159193039, Accuracy: 98.38658905029297\n",
      "Epoch 132, Loss: 0.07963287085294724, Accuracy: 98.44141387939453\n",
      "Epoch 133, Loss: 0.07849061489105225, Accuracy: 98.46491241455078\n",
      "Epoch 134, Loss: 0.0771343857049942, Accuracy: 98.48448944091797\n",
      "Epoch 135, Loss: 0.07584907114505768, Accuracy: 98.48841094970703\n",
      "Epoch 136, Loss: 0.07422827184200287, Accuracy: 98.4962387084961\n",
      "Epoch 137, Loss: 0.07262223958969116, Accuracy: 98.50798797607422\n",
      "Epoch 138, Loss: 0.07090356200933456, Accuracy: 98.56672668457031\n",
      "Epoch 139, Loss: 0.06922190636396408, Accuracy: 98.60197448730469\n",
      "Epoch 140, Loss: 0.06757397949695587, Accuracy: 98.63330078125\n",
      "Epoch 141, Loss: 0.06619498133659363, Accuracy: 98.6724624633789\n",
      "Epoch 142, Loss: 0.06506027281284332, Accuracy: 98.68421173095703\n",
      "Epoch 143, Loss: 0.06382487714290619, Accuracy: 98.71553802490234\n",
      "Epoch 144, Loss: 0.0622161403298378, Accuracy: 98.75469970703125\n",
      "Epoch 145, Loss: 0.060710713267326355, Accuracy: 98.77427673339844\n",
      "Epoch 146, Loss: 0.059709127992391586, Accuracy: 98.82518768310547\n",
      "Epoch 147, Loss: 0.058934763073921204, Accuracy: 98.83301544189453\n",
      "Epoch 148, Loss: 0.05819066986441612, Accuracy: 98.82518768310547\n",
      "Epoch 149, Loss: 0.0568854846060276, Accuracy: 98.82518768310547\n",
      "Epoch 150, Loss: 0.055817071348428726, Accuracy: 98.82127380371094\n",
      "Epoch 151, Loss: 0.05445019528269768, Accuracy: 98.88392639160156\n",
      "Epoch 152, Loss: 0.0531000941991806, Accuracy: 98.90350341796875\n",
      "Epoch 153, Loss: 0.051845159381628036, Accuracy: 98.94266510009766\n",
      "Epoch 154, Loss: 0.05031771957874298, Accuracy: 98.97791290283203\n",
      "Epoch 155, Loss: 0.04905733838677406, Accuracy: 98.98182678222656\n",
      "Epoch 156, Loss: 0.0477650910615921, Accuracy: 99.00140380859375\n",
      "Epoch 157, Loss: 0.04653850942850113, Accuracy: 99.02881622314453\n",
      "Epoch 158, Loss: 0.04558715969324112, Accuracy: 99.05622863769531\n",
      "Epoch 159, Loss: 0.044538453221321106, Accuracy: 99.0836410522461\n",
      "Epoch 160, Loss: 0.04336933791637421, Accuracy: 99.11105346679688\n",
      "Epoch 161, Loss: 0.042415063828229904, Accuracy: 99.1306381225586\n",
      "Epoch 162, Loss: 0.041509442031383514, Accuracy: 99.15021514892578\n",
      "Epoch 163, Loss: 0.04070642963051796, Accuracy: 99.17371368408203\n",
      "Epoch 164, Loss: 0.040141649544239044, Accuracy: 99.19329071044922\n",
      "Epoch 165, Loss: 0.03890817612409592, Accuracy: 99.21678924560547\n",
      "Epoch 166, Loss: 0.03823576122522354, Accuracy: 99.19721221923828\n",
      "Epoch 167, Loss: 0.037873491644859314, Accuracy: 99.22462463378906\n",
      "Epoch 168, Loss: 0.03639117255806923, Accuracy: 99.27944946289062\n",
      "Epoch 169, Loss: 0.03562212362885475, Accuracy: 99.29903411865234\n",
      "Epoch 170, Loss: 0.03456995263695717, Accuracy: 99.34210968017578\n",
      "Epoch 171, Loss: 0.033236339688301086, Accuracy: 99.33036041259766\n",
      "Epoch 172, Loss: 0.03221679478883743, Accuracy: 99.3538589477539\n",
      "Epoch 173, Loss: 0.031163236126303673, Accuracy: 99.39693450927734\n",
      "Epoch 174, Loss: 0.030442537739872932, Accuracy: 99.41651153564453\n",
      "Epoch 175, Loss: 0.029535306617617607, Accuracy: 99.43609619140625\n",
      "Epoch 176, Loss: 0.028765056282281876, Accuracy: 99.45958709716797\n",
      "Epoch 177, Loss: 0.027787990868091583, Accuracy: 99.49483489990234\n",
      "Epoch 178, Loss: 0.026894500479102135, Accuracy: 99.53399658203125\n",
      "Epoch 179, Loss: 0.026201369240880013, Accuracy: 99.54573822021484\n",
      "Epoch 180, Loss: 0.02551502175629139, Accuracy: 99.55357360839844\n",
      "Epoch 181, Loss: 0.024780776351690292, Accuracy: 99.59273529052734\n",
      "Epoch 182, Loss: 0.02393248863518238, Accuracy: 99.59664916992188\n",
      "Epoch 183, Loss: 0.023360220715403557, Accuracy: 99.6083984375\n",
      "Epoch 184, Loss: 0.022628791630268097, Accuracy: 99.63189697265625\n",
      "Epoch 185, Loss: 0.022051768377423286, Accuracy: 99.65930938720703\n",
      "Epoch 186, Loss: 0.021450642496347427, Accuracy: 99.6671371459961\n",
      "Epoch 187, Loss: 0.020796576514840126, Accuracy: 99.72196197509766\n",
      "Epoch 188, Loss: 0.020222781226038933, Accuracy: 99.71412658691406\n",
      "Epoch 189, Loss: 0.019605789333581924, Accuracy: 99.70238494873047\n",
      "Epoch 190, Loss: 0.019553806632757187, Accuracy: 99.71804809570312\n",
      "Epoch 191, Loss: 0.0189418476074934, Accuracy: 99.73371124267578\n",
      "Epoch 192, Loss: 0.01819406822323799, Accuracy: 99.75720977783203\n",
      "Epoch 193, Loss: 0.017531244084239006, Accuracy: 99.78853607177734\n",
      "Epoch 194, Loss: 0.01667032763361931, Accuracy: 99.80028533935547\n",
      "Epoch 195, Loss: 0.016284571960568428, Accuracy: 99.7963638305664\n",
      "Epoch 196, Loss: 0.015507395379245281, Accuracy: 99.80419921875\n",
      "Epoch 197, Loss: 0.01498168520629406, Accuracy: 99.81202697753906\n",
      "Epoch 198, Loss: 0.01457271073013544, Accuracy: 99.83161163330078\n",
      "Epoch 199, Loss: 0.014251534827053547, Accuracy: 99.83552551269531\n",
      "Epoch 200, Loss: 0.013771607540547848, Accuracy: 99.85902404785156\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    for seqs, labels in train_ds:\n",
    "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
    "    print(template.format(epoch + 1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100))\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "q:  ['여기 기프티콘 되죠 \\n']\n",
      "a:  ['\\t 네 현금영수증 해드릴까 요 \\n']\n",
      "p:  ['여기 진동 벨 로 알려 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['네 에 테이크 아웃 도 가능한가요 \\n']\n",
      "a:  ['\\t 네 로 오시 면 테이크 아웃 잔 에 담아 드려요 \\n']\n",
      "p:  ['아뇨 현재 법적 으로 금지 하고 있어요 \\n']\n",
      "_\n",
      "q:  ['아메리카노 톨 사이즈 로 주세요 \\n']\n",
      "a:  ['\\t 따뜻한 거 로 드릴 까요 \\n']\n",
      "p:  ['둘 다 아이스 인가요 \\n']\n",
      "_\n",
      "q:  ['진동 을 따로 주시나요 \\n']\n",
      "a:  ['\\t 주 번호 로 드리겠습니다 \\n']\n",
      "p:  ['네 기다리시면 안내 해드릴게요 \\n']\n",
      "_\n",
      "q:  ['자리 있나요 \\n']\n",
      "a:  ['\\t 네 있습니다 \\n']\n",
      "p:  ['네 배달 비 3000원 입니다 \\n']\n",
      "_\n",
      "q:  ['그럼 루이보스 밀크 티 하나 \\n']\n",
      "a:  ['\\t 네 알겠습니다 \\n']\n",
      "p:  ['네 디카 페인 라테 아메리카노 주문 가능합니다 \\n']\n",
      "_\n",
      "q:  ['다음 에 무료 로 하고 엔 도장 찍어주세요 \\n']\n",
      "a:  ['\\t 네 \\n']\n",
      "p:  ['네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n",
      "_\n",
      "q:  ['아메리카노 한 잔 에 얼마 죠 \\n']\n",
      "a:  ['\\t 입니다 \\n']\n",
      "p:  ['4000원 입니다 \\n']\n",
      "_\n",
      "q:  ['얼마나 \\n']\n",
      "a:  ['\\t 바로 만들어 드릴게요 \\n']\n",
      "p:  ['네 고객 님 결제 완료 되었습니다 \\n']\n",
      "_\n",
      "q:  ['카푸치노 는 로 주시 고 아메리카노 는 로 \\n']\n",
      "a:  ['\\t 네 더 없으세요 \\n']\n",
      "p:  ['드시고 가시나요 \\n']\n",
      "_\n",
      "q:  ['아메리카노 는 어떤 종류 가 있나요 \\n']\n",
      "a:  ['\\t 디카 페인 과 기본 아메리카노 2 종류 있습니다 \\n']\n",
      "p:  ['피지 와 나 티 종류 가 있습니다 \\n']\n",
      "_\n",
      "q:  ['카카오 페이 로 결제 가능한가요 \\n']\n",
      "a:  ['\\t 네 가능합니다 \\n']\n",
      "p:  ['네 2시 까지 가능하십니다 \\n']\n",
      "_\n",
      "q:  ['오늘 의 커피 는 커피 로 하나요 맛 이 \\n']\n",
      "a:  ['\\t 아 네 오늘 은 과테말라 커피 입니다 \\n']\n",
      "p:  ['루이보스 아쌈 이렇게 두 개 있습니다 \\n']\n",
      "_\n",
      "q:  ['머핀 은 뭐 가 제일 \\n']\n",
      "a:  ['\\t 블루베리 머핀 이 잘 나갑니다 \\n']\n",
      "p:  ['네 고객 님 티 종류 다 아이스 는 하프 디카 페인 중 에 선택 하실 수 있습니다 \\n']\n",
      "_\n",
      "q:  ['현금 영수증 해주세요 \\n']\n",
      "a:  ['\\t 네 번호 찍어주세요 \\n']\n",
      "p:  ['네 번호 찍어주세요 \\n']\n",
      "_\n",
      "q:  ['둘 다 톨 사이즈 로 주세요 \\n']\n",
      "a:  ['\\t 여기 서 드시고 요 \\n']\n",
      "p:  ['네 티라미수 는 4500원 입니다 \\n']\n",
      "_\n",
      "q:  ['아이스 아메리카노 한 잔 가능한가요 \\n']\n",
      "a:  ['\\t 네 가능합니다 \\n']\n",
      "p:  ['네 고객 님 티 종류 다 아이스 가능합니다 \\n']\n",
      "_\n",
      "q:  ['아이스 아메리카노 에 샷 이 몇 개 \\n']\n",
      "a:  ['\\t 아이스 아메리카노 에 샷 은 개 \\n']\n",
      "p:  ['사용 하시고 잔액 3천 원 인데 충전 하시겠어요 \\n']\n",
      "_\n",
      "q:  ['카페라테 한 잔 주세요 \\n']\n",
      "a:  ['\\t 카페라테 따뜻한 걸 로 드릴 까요 \\n']\n",
      "p:  ['네 카페라떼 컵 사이즈 는 뭘 로 드릴 까요 \\n']\n",
      "_\n",
      "q:  ['아니요 \\n']\n",
      "a:  ['\\t 네 더 필요하신 건 없으신 가요 \\n']\n",
      "p:  ['네 번호 입력 해주세요 \\n']\n",
      "_\n",
      "q:  ['네 찍어주세요 \\n']\n",
      "a:  ['\\t 네 주문 딸기 스무디 와 쿠키 드릴게요 \\n']\n",
      "p:  ['네 이리 주세요 \\n']\n",
      "_\n",
      "q:  ['시즌 메뉴 오늘 도 가능한가요 \\n']\n",
      "a:  ['\\t 네 시즌 메뉴 가능합니다 \\n']\n",
      "p:  ['네 500원 할인 되세요 \\n']\n",
      "_\n",
      "q:  ['시즌 메뉴 와 함께 되어 있는 세트 메뉴 가 있나요 \\n']\n",
      "a:  ['\\t 네 치즈 케이크 와 시즌 메뉴 두 잔 으로 세트 메뉴 있습니다 \\n']\n",
      "p:  ['네 유효 기간 내 라면 사용 가능하세요 \\n']\n",
      "_\n",
      "q:  ['라테 에 우유 두 도 변경 가능한가요 \\n']\n",
      "a:  ['\\t 네 라테 에 두유 로 변경 가능합니다 \\n']\n",
      "p:  ['치즈 케이크 가 잘 어울려서 같이 주문 하시는 고객 님 들이 많으세요 \\n']\n",
      "_\n",
      "q:  ['네 먹고 갈 거 예요 \\n']\n",
      "a:  ['\\t 카드 여기 주세요 \\n']\n",
      "p:  ['가실 때 말씀 해주시면 테이크 아웃 잔 에 드릴 테 니 우선 머그잔 에 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['카페인 이 음료 있나요 \\n']\n",
      "a:  ['\\t 티 음료 와 스무디 에는 카페인 이 않습니다 \\n']\n",
      "p:  ['피지 와 나 티 종류 가 있습니다 \\n']\n",
      "_\n",
      "q:  ['딸기스무디 랑 키위 스무디 는 생 과일 인가요 \\n']\n",
      "a:  ['\\t 딸기 는 키위 는 생 과일 을 사용 하고 있습니다 \\n']\n",
      "p:  ['9시 부터 열어요 \\n']\n",
      "_\n",
      "q:  ['그럼 딸기 스무디 하나 주세요 \\n']\n",
      "a:  ['\\t 드시고 가시나요 \\n']\n",
      "p:  ['아이스 아메리카노 4000원 입니다 \\n']\n",
      "_\n",
      "q:  ['아메리카노 한 잔이요 \\n']\n",
      "a:  ['\\t 아이스 아메리카노 로 드릴 까요 \\n']\n",
      "p:  ['따뜻한 걸 로 드릴 까요 \\n']\n",
      "_\n",
      "q:  ['네 도 같이 \\n']\n",
      "a:  ['\\t 네 아메리카노 4000원 입니다 \\n']\n",
      "p:  ['네 번호 찍어주세요 \\n']\n",
      "_\n",
      "q:  ['디카 페인 아이스 아메리카노 한 잔 주세요 \\n']\n",
      "a:  ['\\t 디카 페인 아이스 아메리카노 는 기존 금액 에 300원 추가 되는데 괜찮으신 가요 \\n']\n",
      "p:  ['아이스 아메리카노 포장 이신 가요 \\n']\n",
      "_\n",
      "q:  ['커피 음료 것 뭐 가 있나요 \\n']\n",
      "a:  ['\\t 스무디 와 주스 있습니다 \\n']\n",
      "p:  ['기프티콘 으로 대체 할 수 있는 시즌 메뉴 는 없습니다 \\n']\n",
      "_\n",
      "q:  ['주스 어떤 종류 있나요 \\n']\n",
      "a:  ['\\t 딸기 주스 주스 주스 가 있습니다 \\n']\n",
      "p:  ['네 이 쪽 에 있습니다 \\n']\n",
      "_\n",
      "q:  ['플랫 화이트 라지 로 주세요 \\n']\n",
      "a:  ['\\t 네 \\n']\n",
      "p:  ['쿠폰 주시 면 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['네 레드 벨벳 케이크 주세요 \\n']\n",
      "a:  ['\\t 음료 는 뭘 로 드릴 까요 \\n']\n",
      "p:  ['네 진동 벨 이 울리면 픽업 대로 와주세요 \\n']\n",
      "_\n",
      "q:  ['네 먹고 갈 거 예요 \\n']\n",
      "a:  ['\\t 유리잔 괜찮으세요 \\n']\n",
      "p:  ['가실 때 말씀 해주시면 테이크 아웃 잔 에 드릴 테 니 우선 머그잔 에 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['따뜻한 밀크 티 주세요 \\n']\n",
      "a:  ['\\t 네 \\n']\n",
      "p:  ['4천원 입니다 \\n']\n",
      "_\n",
      "q:  ['음료 얼마나 하나요 \\n']\n",
      "a:  ['\\t 10분 정도 주시 면 됩니다 \\n']\n",
      "p:  ['네 총 9500원 결제 해드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['아이스 아메리카노 한잔 얼마 인가요 \\n']\n",
      "a:  ['\\t 4500원 입니다 \\n']\n",
      "p:  ['5000원 입니다 \\n']\n",
      "_\n",
      "q:  ['현금영수증 번호 \\n']\n",
      "a:  ['\\t 네 \\n']\n",
      "p:  ['네 알겠습니다 \\n']\n",
      "_\n",
      "q:  ['이 카드 로 결제 해주세요 \\n']\n",
      "a:  ['\\t 네 결제 도 와 드릴게요 \\n']\n",
      "p:  ['네 감사합니다 \\n']\n",
      "_\n",
      "q:  ['주문 한 게 다 안 \\n']\n",
      "a:  ['\\t 주 번호 가 몇 이 죠 \\n']\n",
      "p:  ['여기 진동 벨 가지 고 계시다가 울리면 주문 한 음료 가져가세요 \\n']\n",
      "_\n",
      "q:  ['을 \\n']\n",
      "a:  ['\\t \\n']\n",
      "p:  ['와이파이 비밀번호 는 종이 에 써져있습니다 \\n']\n",
      "_\n",
      "q:  ['베이글 은 얼마 인가요 \\n']\n",
      "a:  ['\\t 베이글 은 2000원 입니다 \\n']\n",
      "p:  ['플랫 화이트 4500원 입니다 \\n']\n",
      "_\n",
      "q:  ['지금 되나요 \\n']\n",
      "a:  ['\\t 는 계절 메뉴 라 지금 은 판매 하지 않습니다 \\n']\n",
      "p:  ['네 2시 까지 가능하십니다 \\n']\n",
      "_\n",
      "q:  ['바닐라 라테 는 따뜻하게 주세요 \\n']\n",
      "a:  ['\\t 네 적립 이나 할인 카드 있으세요 \\n']\n",
      "p:  ['4천원 입니다 \\n']\n",
      "_\n",
      "q:  ['테이크 아웃 으로 부탁드립니다 \\n']\n",
      "a:  ['\\t 결제 는 이 쪽 에서 도 와 드릴게요 \\n']\n",
      "p:  ['네 있습니다 \\n']\n",
      "_\n",
      "q:  ['혹시 테이크 아웃 잔 에 수 있나요 \\n']\n",
      "a:  ['\\t 테이크 아웃 하시는 건가 요 \\n']\n",
      "p:  ['네 캐리어 에 담아 드릴게요 \\n']\n",
      "_\n",
      "q:  ['아메리카노 하나 는 샷 추가 해주세요 \\n']\n",
      "a:  ['\\t 아메리카노 는 둘 다 따뜻한 걸 로 드릴 까요 \\n']\n",
      "p:  ['아메리카노 따뜻한 건가 요 \\n']\n",
      "_\n",
      "q:  ['쿠폰 찍어주세요 \\n']\n",
      "a:  ['\\t 네 찍어 드릴게요 \\n']\n",
      "p:  ['10 개 다모아 오시 면 커피한잔 드려요 \\n']\n",
      "_\n",
      "q:  ['주문 할게요 \\n']\n",
      "a:  ['\\t 어떤 거 드릴 까요 \\n']\n",
      "p:  ['네 알겠습니다 \\n']\n",
      "_\n",
      "q:  ['파나요 \\n']\n",
      "a:  ['\\t 는 계절 지금 은 \\n']\n",
      "p:  ['네 고객 님 4500원 입니다 \\n']\n",
      "_\n",
      "q:  ['그럼 겨울 메뉴 뭐 가 \\n']\n",
      "a:  ['\\t 겨울 엔 감귤 라테 가 제일 많이 나가요 \\n']\n",
      "p:  ['스콘 은 데워 드릴 까요 \\n']\n",
      "_\n",
      "q:  ['네 결제 는 카드 로 할게요 \\n']\n",
      "a:  ['\\t 네 결제 완료 되었습니다 \\n']\n",
      "p:  ['네 번호 찍어주세요 \\n']\n",
      "_\n",
      "q:  ['둘 다 사이즈 로 할게요 \\n']\n",
      "a:  ['\\t 네 결제 는 어떤 것 으로 도 와 드릴 까요 \\n']\n",
      "p:  ['네 포인트 적립 하시나요 \\n']\n",
      "_\n",
      "q:  ['기프티콘 으로 결제 할게요 \\n']\n",
      "a:  ['\\t 네 그럼 쿠폰 저 \\n']\n",
      "p:  ['저 한테 보여주시고 제 가 확인 버튼 누르면 돼요 \\n']\n",
      "_\n",
      "q:  ['녹차 라테 1 잔 주세요 \\n']\n",
      "a:  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n",
      "p:  ['카페모카 위 에 휘핑 올려 드릴 까요 \\n']\n",
      "_\n",
      "q:  ['네 그럼 휘핑크림 추가 해서 주세요 \\n']\n",
      "a:  ['\\t 네 녹차 라테 에 휘핑크림 추가 해서 4500원 입니다 \\n']\n",
      "p:  ['아이스 아메리카노 4000원 입니다 \\n']\n",
      "_\n",
      "q:  ['브레드 종류 는 뭐 가 있나요 \\n']\n",
      "a:  ['\\t 허니 브레드 와 갈릭 치즈 브레드 가 있습니다 \\n']\n",
      "p:  ['네 치즈케이크 는 지금 다 팔렸습니다 \\n']\n",
      "_\n",
      "q:  ['생크림 이 건 어떤 건가 요 \\n']\n",
      "a:  ['\\t 허니 브레드 입니다 \\n']\n",
      "p:  ['네 고객 님 사이즈 는 톨 사이즈 괜찮으신 가요 \\n']\n",
      "_\n",
      "q:  ['네 그렇게 만들어 주세요 \\n']\n",
      "a:  ['\\t 더 필요한 건 없으세요 \\n']\n",
      "p:  ['먹고 가시는 거 면 유리잔 괜찮으실까 요 \\n']\n",
      "_\n",
      "q:  ['여기 있습니다 \\n']\n",
      "a:  ['\\t 네 확인 되셨고 되면 진동 벨 거 예요 \\n']\n",
      "p:  ['아이스 아메리카노 한잔 화이트 초코 머핀 1 개 주문 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['핫초코 한 잔 아메리카노 사이 즈 업 한 잔 하면 얼마 인가요 \\n']\n",
      "a:  ['\\t 입니다 \\n']\n",
      "p:  ['9500원 입니다 \\n']\n",
      "_\n",
      "q:  ['주스 는 다른 건 없나요 \\n']\n",
      "a:  ['\\t 그럼 에 라테 추천 \\n']\n",
      "p:  ['네 치즈케이크 는 지금 다 팔렸습니다 \\n']\n",
      "_\n",
      "q:  ['그건 \\n']\n",
      "a:  ['\\t 네 만 따듯 해 요 \\n']\n",
      "p:  ['네 번호 입력 해주세요 \\n']\n",
      "_\n",
      "q:  ['통신사 할인 되죠 \\n']\n",
      "a:  ['\\t 네 300원 할인 됩니다 \\n']\n",
      "p:  ['24시간 영업 하고 있어요 \\n']\n",
      "_\n",
      "q:  ['매장 에서 언제 까지 영업 하시나요 \\n']\n",
      "a:  ['\\t 오후 10시 까지 영업 입니다 \\n']\n",
      "p:  ['네 아메리카노 만 사이즈 업 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['아니요 그냥 주세요 \\n']\n",
      "a:  ['\\t 결제 해드릴게요 \\n']\n",
      "p:  ['카페모카 5천 원 입니다 \\n']\n",
      "_\n",
      "q:  ['가격 안 되나요 \\n']\n",
      "a:  ['\\t 한 해드릴게요 \\n']\n",
      "p:  ['네 바닥 청소 도 해야 해서 죄송합니다 \\n']\n",
      "_\n",
      "q:  ['카페라테 한잔 주세요 \\n']\n",
      "a:  ['\\t 따뜻한 걸 로 드릴 까요 \\n']\n",
      "p:  ['네 진동 벨 울리면 찾으러 오세요 \\n']\n",
      "_\n",
      "q:  ['네 차가운 걸 로 주세요 \\n']\n",
      "a:  ['\\t 4500원 입니다 \\n']\n",
      "p:  ['드시고 가시나요 \\n']\n",
      "_\n",
      "q:  ['어떤 게 괜찮아요 \\n']\n",
      "a:  ['\\t 이나 원두 를 하시는 게 아니면 예 가체 많이 추천 \\n']\n",
      "p:  ['네 고객 님 포인트 적립 하시겠어요 \\n']\n",
      "_\n",
      "q:  ['그럼 추천 치즈 케이크 도 같이 주세요 \\n']\n",
      "a:  ['\\t 네 매장 에서 드시고 가시나요 \\n']\n",
      "p:  ['네 드시고 가시나요 \\n']\n",
      "_\n",
      "q:  ['그리고 휘핑크림 은 에스프레소 크림 으로 \\n']\n",
      "a:  ['\\t 결제 는 어떻게 해드릴까 요 \\n']\n",
      "p:  ['네 가능합니다 \\n']\n",
      "_\n",
      "q:  ['지금 도 할인 하나요 \\n']\n",
      "a:  ['\\t 네 10시 까지 하고 있습니다 \\n']\n",
      "p:  ['네 계산 도 와 드릴게요 \\n']\n",
      "_\n",
      "q:  ['그럼 와 아이스 아메리카노 로 할게요 \\n']\n",
      "a:  ['\\t 더 필요하신 건 없나요 \\n']\n",
      "p:  ['네 잠시 만 기다려주세요 \\n']\n",
      "_\n",
      "q:  ['네 할인 적립 은 \\n']\n",
      "a:  ['\\t 네 바코드 \\n']\n",
      "p:  ['네 알겠습니다 \\n']\n",
      "_\n",
      "q:  ['초코 프라푸치노 주세요 \\n']\n",
      "a:  ['\\t 휘핑 올려 드릴 까요 \\n']\n",
      "p:  ['네 진동 벨 로 알려 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['시럽 도 가 \\n']\n",
      "a:  ['\\t 드시고 가시나요 \\n']\n",
      "p:  ['네 번호 입력 해주세요 \\n']\n",
      "_\n",
      "q:  ['둘 다 사이즈 로 주세요 \\n']\n",
      "a:  ['\\t 드시고 가시나요 \\n']\n",
      "p:  ['신 맛 는 방금 전 에 다 떨어졌어요 \\n']\n",
      "_\n",
      "q:  ['마시다가 갈 건데 테이크아웃 으로 주세요 \\n']\n",
      "a:  ['\\t 상 매장 에서는 머그컵 으로 드리고 있어요 \\n']\n",
      "p:  ['앉아계실 거 면 컵 만 가능하세요 \\n']\n",
      "_\n",
      "q:  ['나갈 때 테이크아웃 컵 으로 수 있나요 \\n']\n",
      "a:  ['\\t 네 그렇게 해드릴게요 \\n']\n",
      "p:  ['네 저기 서 직접 가져오시면 됩니다 \\n']\n",
      "_\n",
      "q:  ['아메리카노 두 잔 한잔 주세요 \\n']\n",
      "a:  ['\\t 드시고 가실 건가 요 \\n']\n",
      "p:  ['둘 다 아이스 인가요 \\n']\n",
      "_\n",
      "q:  ['얼마나 하나요 \\n']\n",
      "a:  ['\\t 5분 정도 \\n']\n",
      "p:  ['네 고객 님 결제 완료 되었습니다 \\n']\n",
      "_\n",
      "q:  ['포인트 적립 해주세요 \\n']\n",
      "a:  ['\\t 네 번호 입력 부탁드립니다 \\n']\n",
      "p:  ['네 카드 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['네 번호 로 할게요 \\n']\n",
      "a:  ['\\t 네 에 번호 \\n']\n",
      "p:  ['네 고객 님 포인트 적립 하시겠어요 \\n']\n",
      "_\n",
      "q:  ['아 포인트 포인트 사용 해주세요 \\n']\n",
      "a:  ['\\t 네 고객 님 포인트 총 있으신 데 사용 도 와 드리겠습니다 \\n']\n",
      "p:  ['네 가능해요 \\n']\n",
      "_\n",
      "q:  ['톨 사이즈 로 주문 할게요 \\n']\n",
      "a:  ['\\t 네 계산 도 와 드리겠습니다 \\n']\n",
      "p:  ['네 알겠습니다 \\n']\n",
      "_\n",
      "q:  ['아메리카노 사이즈 가능한가요 \\n']\n",
      "a:  ['\\t 네 500원 만 추가 하시면 가능하십니다 \\n']\n",
      "p:  ['네 500원 할인 되세요 \\n']\n",
      "_\n",
      "q:  ['사이 즈 업 해서 주세요 \\n']\n",
      "a:  ['\\t 네 결제 는 어떻게 도 와 드릴 까요 \\n']\n",
      "p:  ['아메리카노 기프티콘 사용 입니다 \\n']\n",
      "_\n",
      "q:  ['커피 는 텀블러 에 담아주세요 \\n']\n",
      "a:  ['\\t 네 텀블러 할인 4000원 결제 도 와 드리겠습니다 \\n']\n",
      "p:  ['텀블러 할인 300원 같이 해드릴게요 \\n']\n",
      "_\n",
      "q:  ['아니요 아이스 로 주세요 \\n']\n",
      "a:  ['\\t 드시고 가실 건가 요 \\n']\n",
      "p:  ['레귤러 사이즈 로 괜찮으세요 \\n']\n",
      "_\n",
      "q:  ['테이크아웃 할게요 \\n']\n",
      "a:  ['\\t 지금 중 인데 케이크 주문 하시면 아메리카노 한잔 로 드려요 \\n']\n",
      "p:  ['네 알겠습니다 \\n']\n",
      "_\n",
      "q:  ['현금 결제 가 안 \\n']\n",
      "a:  ['\\t 현금 은 에서 주문 도 와 드리겠습니다 \\n']\n",
      "p:  ['네 더 필요한 건 없으세요 \\n']\n",
      "_\n",
      "q:  ['포인트 적립 되나요 \\n']\n",
      "a:  ['\\t 번호 포인트 적립 도 와 드리고 있어요 \\n']\n",
      "p:  ['네 됩니다 \\n']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_\n",
      "q:  ['포인트 적립 할게요 \\n']\n",
      "a:  ['\\t 네 결제 되셨습니다 \\n']\n",
      "p:  ['네 카드 도 와 드리겠습니다 \\n']\n",
      "_\n",
      "q:  ['티라미수 는 있나요 \\n']\n",
      "a:  ['\\t 네 티라미수 는 있습니다 \\n']\n",
      "p:  ['네 고객 님 티 종류 다 아이스 가능합니다 \\n']\n",
      "_\n",
      "q:  ['네 현금영수증 해주세요 \\n']\n",
      "a:  ['\\t 네 드시고 가시나요 \\n']\n",
      "p:  ['네 번호 입력 해주세요 \\n']\n",
      "_\n",
      "q:  ['샷 추가 해주세요 \\n']\n",
      "a:  ['\\t 네 알겠습니다 \\n']\n",
      "p:  ['네 디카 페인 라테 아메리카노 주문 가능합니다 \\n']\n",
      "_\n",
      "q:  ['얼마 에요 \\n']\n",
      "a:  ['\\t 만 원 입니다 \\n']\n",
      "p:  ['베이글 과 동일하게 2000원 입니다 \\n']\n",
      "_\n",
      "q:  ['아이스 아메리카노 랑 샌드위치 주세요 \\n']\n",
      "a:  ['\\t 10시 에 세트 할인 가능하세요 \\n']\n",
      "p:  ['아이스 아메리카노 4000원 입니다 \\n']\n"
     ]
    }
   ],
   "source": [
    "for test_seq, test_labels in test_ds:\n",
    "    prediction = test_step(model, test_seq)\n",
    "    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
    "    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
    "    texts = tokenizer.sequences_to_texts(prediction.numpy())\n",
    "    print('_')\n",
    "    print('q: ', test_text)\n",
    "    print('a: ', gt_text)\n",
    "    print('p: ', texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
